{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random, numpy as jnp\n",
    "import numpy as np\n",
    "from jax.tree_util import tree_map\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split, default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = x.reshape((-1, 28*28))\n",
    "        x = nn.Dense(128)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(64)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(10)(x)\n",
    "        return x\n",
    "    \n",
    "def create_train_state(rng, learning_rate):\n",
    "    model = MLP()\n",
    "    params = model.init(rng, jnp.ones((1, 28*28)))\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply, params=params, tx=optimizer\n",
    "    )\n",
    "\n",
    "# got the dataloader superclassing stuff from jax documentation\n",
    "def numpy_collate(batch):\n",
    "    return tree_map(np.asarray, default_collate(batch))\n",
    "\n",
    "class NumpyLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size=1,\n",
    "                shuffle=False, sampler=None,\n",
    "                batch_sampler=None, num_workers=0,\n",
    "                pin_memory=False, drop_last=False,\n",
    "                timeout=0, worker_init_fn=None):\n",
    "        super(self.__class__, self).__init__(dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=drop_last,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn)\n",
    "\n",
    "class FlattenAndCast(object):\n",
    "    def __call__(self, pic):\n",
    "        return np.ravel(np.array(pic, dtype=jnp.float32))\n",
    "# here ends\n",
    "  \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    FlattenAndCast()\n",
    "    ]) # maybe play with normalization here\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_dataset, val_dataset = random_split(train_dataset, [55000, 5000])\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = NumpyLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = NumpyLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = NumpyLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging Interval - Train Epoch: 1 Batch Idx: 0 \tLoss: 2.337771\n",
      "Logging Interval - Train Epoch: 1 Batch Idx: 200 \tLoss: 0.282386\n",
      "Logging Interval - Train Epoch: 1 Batch Idx: 400 \tLoss: 0.161419\n",
      "Logging Interval - Train Epoch: 1 Batch Idx: 600 \tLoss: 0.075765\n",
      "Logging Interval - Train Epoch: 1 Batch Idx: 800 \tLoss: 0.152743\n",
      "Logging Interval - Train Epoch: 1 Batch Idx: 1000 \tLoss: 0.196576\n",
      "Logging Interval - Train Epoch: 1 Batch Idx: 1200 \tLoss: 0.045747\n",
      "Logging Interval - Train Epoch: 1 Batch Idx: 1400 \tLoss: 0.237803\n",
      "Logging Interval - Train Epoch: 1 Batch Idx: 1600 \tLoss: 0.091024\n",
      "Logging Interval - Train Epoch: 2 Batch Idx: 0 \tLoss: 0.018625\n",
      "Logging Interval - Train Epoch: 2 Batch Idx: 200 \tLoss: 0.041462\n",
      "Logging Interval - Train Epoch: 2 Batch Idx: 400 \tLoss: 0.072460\n",
      "Logging Interval - Train Epoch: 2 Batch Idx: 600 \tLoss: 0.046142\n",
      "Logging Interval - Train Epoch: 2 Batch Idx: 800 \tLoss: 0.009624\n",
      "Logging Interval - Train Epoch: 2 Batch Idx: 1000 \tLoss: 0.066928\n",
      "Logging Interval - Train Epoch: 2 Batch Idx: 1200 \tLoss: 0.041265\n",
      "Logging Interval - Train Epoch: 2 Batch Idx: 1400 \tLoss: 0.217380\n",
      "Logging Interval - Train Epoch: 2 Batch Idx: 1600 \tLoss: 0.073431\n",
      "Logging Interval - Train Epoch: 3 Batch Idx: 0 \tLoss: 0.060027\n",
      "Logging Interval - Train Epoch: 3 Batch Idx: 200 \tLoss: 0.083092\n",
      "Logging Interval - Train Epoch: 3 Batch Idx: 400 \tLoss: 0.062094\n",
      "Logging Interval - Train Epoch: 3 Batch Idx: 600 \tLoss: 0.078636\n",
      "Logging Interval - Train Epoch: 3 Batch Idx: 800 \tLoss: 0.026166\n",
      "Logging Interval - Train Epoch: 3 Batch Idx: 1000 \tLoss: 0.029552\n",
      "Logging Interval - Train Epoch: 3 Batch Idx: 1200 \tLoss: 0.018528\n",
      "Logging Interval - Train Epoch: 3 Batch Idx: 1400 \tLoss: 0.032646\n",
      "Logging Interval - Train Epoch: 3 Batch Idx: 1600 \tLoss: 0.088525\n",
      "Logging Interval - Train Epoch: 4 Batch Idx: 0 \tLoss: 0.019820\n",
      "Logging Interval - Train Epoch: 4 Batch Idx: 200 \tLoss: 0.020895\n",
      "Logging Interval - Train Epoch: 4 Batch Idx: 400 \tLoss: 0.009876\n",
      "Logging Interval - Train Epoch: 4 Batch Idx: 600 \tLoss: 0.162599\n",
      "Logging Interval - Train Epoch: 4 Batch Idx: 800 \tLoss: 0.004524\n",
      "Logging Interval - Train Epoch: 4 Batch Idx: 1000 \tLoss: 0.001975\n",
      "Logging Interval - Train Epoch: 4 Batch Idx: 1200 \tLoss: 0.071040\n",
      "Logging Interval - Train Epoch: 4 Batch Idx: 1400 \tLoss: 0.220587\n",
      "Logging Interval - Train Epoch: 4 Batch Idx: 1600 \tLoss: 0.170939\n",
      "Logging Interval - Train Epoch: 5 Batch Idx: 0 \tLoss: 0.006216\n",
      "Logging Interval - Train Epoch: 5 Batch Idx: 200 \tLoss: 0.086101\n",
      "Logging Interval - Train Epoch: 5 Batch Idx: 400 \tLoss: 0.114684\n",
      "Logging Interval - Train Epoch: 5 Batch Idx: 600 \tLoss: 0.015045\n",
      "Logging Interval - Train Epoch: 5 Batch Idx: 800 \tLoss: 0.063970\n",
      "Logging Interval - Train Epoch: 5 Batch Idx: 1000 \tLoss: 0.001018\n",
      "Logging Interval - Train Epoch: 5 Batch Idx: 1200 \tLoss: 0.001084\n",
      "Logging Interval - Train Epoch: 5 Batch Idx: 1400 \tLoss: 0.050589\n",
      "Logging Interval - Train Epoch: 5 Batch Idx: 1600 \tLoss: 0.027636\n",
      "Validation set: Average loss: 0.0734, Accuracy: 97.70%\n"
     ]
    }
   ],
   "source": [
    "rng = random.PRNGKey(12)\n",
    "state = create_train_state(rng, learning_rate=0.001)\n",
    "\n",
    "def validate(state, val_loader):\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    for data, target in val_loader:\n",
    "        target_one_hot = jax.nn.one_hot(target, num_classes=10)\n",
    "        logits = state.apply_fn(state.params, data)\n",
    "        loss = optax.softmax_cross_entropy(logits, target_one_hot).mean()\n",
    "        val_loss += loss.item() * data.shape[0]  # Scale loss by batch size\n",
    "        # Compute accuracy\n",
    "        predicted_class = jnp.argmax(logits, axis=1)\n",
    "        correct += jnp.sum(predicted_class == target)\n",
    "        total_samples += data.shape[0]\n",
    "    val_loss /= total_samples\n",
    "    accuracy = 100. * correct / total_samples\n",
    "    print(f'Validation set: Average loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    data, target = batch\n",
    "    target_one_hot = jax.nn.one_hot(target, num_classes=10)\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn(params, data)\n",
    "        loss = optax.softmax_cross_entropy(logits, target_one_hot).mean()\n",
    "        return loss\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params) # problem\n",
    "    return state.apply_gradients(grads=grads), loss\n",
    "\n",
    "def train(epoch, state, log_interval=200):\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        state, loss = train_step(state, batch)\n",
    "        if batch_idx % log_interval == 0:\n",
    "            loss_val = loss.item() if hasattr(loss, 'item') else loss\n",
    "            print(f'Logging Interval - Train Epoch: {epoch} Batch Idx: {batch_idx} \\tLoss: {loss_val:.6f}')\n",
    "    return state\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    state = train(epoch, state)\n",
    "    #validate(state, val_loader)\n",
    "validate(state, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
