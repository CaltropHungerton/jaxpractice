{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random, numpy as jnp\n",
    "import numpy as np\n",
    "from jax.tree_util import tree_map\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split, default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Conv(features=6, kernel_size=(5, 5), padding=\"SAME\")(x)\n",
    "        x = nn.sigmoid(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = nn.Conv(features=16, kernel_size=(5, 5), padding=\"VALID\")(x)\n",
    "        x = nn.sigmoid(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = x.reshape((-1, 400))\n",
    "        x = nn.Dense(120)(x)\n",
    "        x = nn.sigmoid(x)\n",
    "        x = nn.Dense(84)(x)\n",
    "        x = nn.sigmoid(x)\n",
    "        x = nn.Dense(10)(x)\n",
    "        return x\n",
    "    \n",
    "def create_train_state(rng, learning_rate):\n",
    "    model = LeNet()\n",
    "    params = model.init(rng, jnp.ones((1, 28, 28, 1))) # correct w/ channel dims?\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply, params=params, tx=optimizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_collate(batch):\n",
    "    return tree_map(np.asarray, default_collate(batch))\n",
    "    \n",
    "class NumpyLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size=1,\n",
    "                shuffle=False, sampler=None,\n",
    "                batch_sampler=None, num_workers=0,\n",
    "                pin_memory=False, drop_last=False,\n",
    "                timeout=0, worker_init_fn=None):\n",
    "        super(self.__class__, self).__init__(dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=drop_last,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn)\n",
    "\n",
    "# NOTE that we cannot flatten, it expects (batch_size, 28, 28, 1) shape\n",
    "# this is a difference between the torch conv and the flax conv,\n",
    "# you have to give it inputs with the extra channel dim at the end\n",
    "class Cast(object):\n",
    "    def __call__(self, pic):\n",
    "        return np.array(pic, dtype=jnp.float32)\n",
    "\n",
    "def add_channel_dim(image):\n",
    "    return np.expand_dims(image, -1)\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    Cast(),\n",
    "    add_channel_dim # adds channel dimension so that we can increase channel nums when we convolve\n",
    "])\n",
    "\n",
    "\"\"\"\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_dataset, val_dataset = random_split(train_dataset, [55000, 5000])\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\"\"\"\n",
    "\n",
    "train_dataset = datasets.MNIST(root='../mnistMLP/data', train=True, download=True, transform=transform)\n",
    "train_dataset, val_dataset = random_split(train_dataset, [55000, 5000])\n",
    "test_dataset = datasets.MNIST(root='../mnistMLP/data', train=False, transform=transform)\n",
    "\n",
    "train_loader = NumpyLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = NumpyLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = NumpyLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging Interval - Train Epoch: 1 Batch Idx: 0 \tLoss: 2.328917\n",
      "Logging Interval - Train Epoch: 1 Batch Idx: 200 \tLoss: 2.051440\n",
      "Logging Interval - Train Epoch: 1 Batch Idx: 400 \tLoss: 0.629653\n",
      "Logging Interval - Train Epoch: 1 Batch Idx: 600 \tLoss: 0.301854\n",
      "Logging Interval - Train Epoch: 1 Batch Idx: 800 \tLoss: 0.496858\n",
      "Logging Interval - Train Epoch: 1 Batch Idx: 1000 \tLoss: 0.345888\n",
      "Logging Interval - Train Epoch: 1 Batch Idx: 1200 \tLoss: 0.258772\n",
      "Logging Interval - Train Epoch: 1 Batch Idx: 1400 \tLoss: 0.139123\n",
      "Logging Interval - Train Epoch: 1 Batch Idx: 1600 \tLoss: 0.376808\n",
      "Logging Interval - Train Epoch: 2 Batch Idx: 0 \tLoss: 0.082918\n",
      "Logging Interval - Train Epoch: 2 Batch Idx: 200 \tLoss: 0.178216\n",
      "Logging Interval - Train Epoch: 2 Batch Idx: 400 \tLoss: 0.113553\n",
      "Logging Interval - Train Epoch: 2 Batch Idx: 600 \tLoss: 0.105725\n",
      "Logging Interval - Train Epoch: 2 Batch Idx: 800 \tLoss: 0.131342\n",
      "Logging Interval - Train Epoch: 2 Batch Idx: 1000 \tLoss: 0.122676\n",
      "Logging Interval - Train Epoch: 2 Batch Idx: 1200 \tLoss: 0.118705\n",
      "Logging Interval - Train Epoch: 2 Batch Idx: 1400 \tLoss: 0.114458\n",
      "Logging Interval - Train Epoch: 2 Batch Idx: 1600 \tLoss: 0.032118\n",
      "Logging Interval - Train Epoch: 3 Batch Idx: 0 \tLoss: 0.022691\n",
      "Logging Interval - Train Epoch: 3 Batch Idx: 200 \tLoss: 0.052780\n",
      "Logging Interval - Train Epoch: 3 Batch Idx: 400 \tLoss: 0.237673\n",
      "Logging Interval - Train Epoch: 3 Batch Idx: 600 \tLoss: 0.141802\n",
      "Logging Interval - Train Epoch: 3 Batch Idx: 800 \tLoss: 0.039760\n",
      "Logging Interval - Train Epoch: 3 Batch Idx: 1000 \tLoss: 0.099899\n",
      "Logging Interval - Train Epoch: 3 Batch Idx: 1200 \tLoss: 0.074085\n",
      "Logging Interval - Train Epoch: 3 Batch Idx: 1400 \tLoss: 0.046688\n",
      "Logging Interval - Train Epoch: 3 Batch Idx: 1600 \tLoss: 0.163257\n",
      "Logging Interval - Train Epoch: 4 Batch Idx: 0 \tLoss: 0.055211\n",
      "Logging Interval - Train Epoch: 4 Batch Idx: 200 \tLoss: 0.025697\n",
      "Logging Interval - Train Epoch: 4 Batch Idx: 400 \tLoss: 0.078268\n",
      "Logging Interval - Train Epoch: 4 Batch Idx: 600 \tLoss: 0.192579\n",
      "Logging Interval - Train Epoch: 4 Batch Idx: 800 \tLoss: 0.054648\n",
      "Logging Interval - Train Epoch: 4 Batch Idx: 1000 \tLoss: 0.327163\n",
      "Logging Interval - Train Epoch: 4 Batch Idx: 1200 \tLoss: 0.073822\n",
      "Logging Interval - Train Epoch: 4 Batch Idx: 1400 \tLoss: 0.048008\n",
      "Logging Interval - Train Epoch: 4 Batch Idx: 1600 \tLoss: 0.007451\n",
      "Logging Interval - Train Epoch: 5 Batch Idx: 0 \tLoss: 0.163868\n",
      "Logging Interval - Train Epoch: 5 Batch Idx: 200 \tLoss: 0.027722\n",
      "Logging Interval - Train Epoch: 5 Batch Idx: 400 \tLoss: 0.018210\n",
      "Logging Interval - Train Epoch: 5 Batch Idx: 600 \tLoss: 0.165192\n",
      "Logging Interval - Train Epoch: 5 Batch Idx: 800 \tLoss: 0.017017\n",
      "Logging Interval - Train Epoch: 5 Batch Idx: 1000 \tLoss: 0.065718\n",
      "Logging Interval - Train Epoch: 5 Batch Idx: 1200 \tLoss: 0.052911\n",
      "Logging Interval - Train Epoch: 5 Batch Idx: 1400 \tLoss: 0.066053\n",
      "Logging Interval - Train Epoch: 5 Batch Idx: 1600 \tLoss: 0.039693\n",
      "Validation set: Average loss: 0.0688, Accuracy: 97.88%\n"
     ]
    }
   ],
   "source": [
    "rng = random.PRNGKey(12)\n",
    "state = create_train_state(rng, learning_rate=0.001)\n",
    "\n",
    "def validate(state, val_loader):\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    for data, target in val_loader:\n",
    "        target_one_hot = jax.nn.one_hot(target, num_classes=10)\n",
    "        logits = state.apply_fn(state.params, data)\n",
    "        loss = optax.softmax_cross_entropy(logits, target_one_hot).mean()\n",
    "        val_loss += loss.item() * data.shape[0]  # Scale loss by batch size\n",
    "        # Compute accuracy\n",
    "        predicted_class = jnp.argmax(logits, axis=1)\n",
    "        correct += jnp.sum(predicted_class == target)\n",
    "        total_samples += data.shape[0]\n",
    "    val_loss /= total_samples\n",
    "    accuracy = 100. * correct / total_samples\n",
    "    print(f'Validation set: Average loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    data, target = batch\n",
    "    target_one_hot = jax.nn.one_hot(target, num_classes=10)\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn(params, data)\n",
    "        loss = optax.softmax_cross_entropy(logits, target_one_hot).mean()\n",
    "        return loss\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    return state.apply_gradients(grads=grads), loss\n",
    "\n",
    "def train(epoch, state, log_interval=200):\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        state, loss = train_step(state, batch)\n",
    "        if batch_idx % log_interval == 0:\n",
    "            loss_val = loss.item() if hasattr(loss, 'item') else loss\n",
    "            print(f'Logging Interval - Train Epoch: {epoch} Batch Idx: {batch_idx} \\tLoss: {loss_val:.6f}')\n",
    "    return state\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    state = train(epoch, state)\n",
    "    #validate(state, val_loader)\n",
    "validate(state, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
